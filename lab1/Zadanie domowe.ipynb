{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inżynieria lingwistyczna\n",
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1 - tokenizacja (12 pkt)\n",
    "\n",
    "Jedną z nowoczesnych technik tokenizacji jest BPE - byte-pair encoding [1]. Technika ta polega na podzielenie słów na częste podsłowa (morfemy). W przeciwieństwie do podejść lingwistycznych, wymagających reguł tworzenia morfemów, BPE wyznacza je automatycznie poprzez wyznaczenie najczęstszych przylegających do siebie sekwencji znaków które występują obok siebie.\n",
    "\n",
    "Algorytm przebiega w następujących krokach.\n",
    "1. Podziel wszystkie słowa na symbole (początkowo pojedyncze znaki)\n",
    "2. Wyznacz najczęściej występującą obok siebie parę symboli \n",
    "3. Stwórz nowy symbol będący konkatenacją dwóch najczęstszych symboli.\n",
    "\n",
    "Uwaga 1: każde słowo zakończone jest specjalnym symbolem końca wyrazu.\n",
    "\n",
    "Uwaga 2: tworzenie nowego symbolu nie powoduje usuniecie starego tj. zawsze jednym z możliwych symboli jest pojedynczy znak, ale jeśli można to stosujemy symbol dłuższy.\n",
    "\n",
    "Przykład: korpus w którym występuje ,,ala'' 5 razy i ,,mama 10 razy''\n",
    "1. Dzielimy słowa na symbole ,,a l a END'' ,,m a m a END''  gdzie END jest symbolem końca wyrazu.\n",
    "2. Najczęstsza para obok siebie to ,,m a'' (20) razy\n",
    "3. Nowy symbol ,,ma''\n",
    "4. Nowy podział ,,a l a END'' ,,ma ma END''\n",
    "5. Najczęstsza para ,,ma ma'' (10) razy\n",
    "6. Nowy symbol ,,mama''\n",
    "7. Nowy podział ,,a l a END'' ,,mama END''\n",
    "8. itd.\n",
    "\n",
    "W pliku ,,brown_clusters.tsv'' pierwsza kolumna to identyfikator skupienia (nie używamy w tym zadaniu), druga kolumna to wyrazy, a trzecia to ich liczności w pewnym korpusie tweetów. Zaimplementuj technike BPE na tych słowach.\n",
    "\n",
    "Zaimplementuj algorytm BPE wykonujący `number_of_iterations` iteracji (łączeń symboli).\n",
    "\n",
    "[1] Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In ACL 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ff3b90528fdb50de90c5c946c157e21",
     "grade": false,
     "grade_id": "cell-93d78a28d4e25cbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "brown_df = pd.read_csv('brown_clusters.tsv', sep='\\t', header=0, names=['cluster', 'word', 'count'])\n",
    "\n",
    "\n",
    "def find_pairs(word, multiplier):\n",
    "    pairs_dct = dict()\n",
    "    symbols = word.split(' ')\n",
    "\n",
    "    for i in range(len(symbols) - 1):\n",
    "        key = f\"{symbols[i]} {symbols[i + 1]}\"\n",
    "        pairs_dct[key] = pairs_dct.get(key, 0) + multiplier\n",
    "\n",
    "    return pairs_dct\n",
    "\n",
    "\n",
    "def preform_bpe(data_df, number_of_iterations):\n",
    "    \"\"\"\n",
    "    Funckcja przyjmuje ramkę w formacie analogicznym do obiektu brown_df (wczytany wyżej)\n",
    "     oraz liczbę iteracji.\n",
    "    Wyjściem funkcji powinna być lista słów z poszczególnymi tokenami/symbolami oddzielonymi spacją.\n",
    "    Za znak końca wyrazu przyjmij END. \n",
    "    \"\"\"\n",
    "    data_df['word'] = data_df['word'].fillna('')\n",
    "    data_df['word'] = data_df['word'].apply(lambda word: ' '.join(list(word)) + ' END')\n",
    "\n",
    "    pairs_global_dct = dict()\n",
    "    for index, row in data_df.iterrows():\n",
    "        word_pairs_dct = find_pairs(row['word'], row['count'])\n",
    "        for k in word_pairs_dct.keys():\n",
    "            pairs_global_dct[k] = pairs_global_dct.get(k, 0) + word_pairs_dct[k]\n",
    "\n",
    "    for _ in range(number_of_iterations):\n",
    "        top_pair = max(pairs_global_dct, key=pairs_global_dct.get)\n",
    "        top_pair_merged = top_pair.replace(\" \", \"\")\n",
    "        # print(\"New symbol:\", top_pair)\n",
    "        influenced_df = data_df[data_df['word'].str.contains(top_pair, regex=False)]\n",
    "        data_df['word'] = data_df['word'].str.replace(top_pair, top_pair_merged, regex=False)\n",
    "\n",
    "        for index, row in influenced_df.iterrows():\n",
    "\n",
    "            word_pairs_dct = find_pairs(row['word'], row['count'])\n",
    "            for k in word_pairs_dct.keys():\n",
    "                pairs_global_dct[k] = pairs_global_dct.get(k, 0) - word_pairs_dct[k]\n",
    "\n",
    "            word_pairs_dct = find_pairs(data_df.loc[index, 'word'], row['count'])\n",
    "            for k in word_pairs_dct.keys():\n",
    "                pairs_global_dct[k] = pairs_global_dct.get(k, 0) + word_pairs_dct[k]\n",
    "\n",
    "    return data_df[\"word\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfff70f711bf389f0f1cd969e7c3a413",
     "grade": true,
     "grade_id": "cell-7e952fa8dcd136fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_list_equal\n",
    "\n",
    "data = {'cluster': range(2), 'word': ['ala', 'mama'], 'count': [5, 10]}\n",
    "df = pd.DataFrame(data, columns=['cluster', 'word', 'count'])\n",
    "vocab = preform_bpe(df, 1)\n",
    "assert_list_equal(vocab, ['a l a END', 'ma ma END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spraw aby Twoja implementacja wypisywała kolejne łączone ze sobą symbole i uruchom Twoją funkcję na np. 50 iteracji, obserwując jakie tokeny są tworzone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New symbol: e END\n",
      "New symbol: t END\n",
      "New symbol: s END\n",
      "New symbol: i n\n",
      "New symbol: t h\n",
      "New symbol: d END\n",
      "New symbol: y END\n",
      "New symbol: . END\n",
      "New symbol: o END\n",
      "New symbol: r END\n",
      "New symbol: > END\n",
      "New symbol: a n\n",
      "New symbol: o n\n",
      "New symbol: o u\n",
      "New symbol: g END\n",
      "New symbol: a END\n",
      "New symbol: l END\n",
      "New symbol: in gEND\n",
      "New symbol: < @\n",
      "New symbol: M E\n",
      "New symbol: N T\n",
      "New symbol: I O\n",
      "New symbol: N >END\n",
      "New symbol: <@ ME\n",
      "New symbol: NT IO\n",
      "New symbol: <@ME NTIO\n",
      "New symbol: <@MENTIO N>END\n",
      "New symbol: r e\n",
      "New symbol: i END\n",
      "New symbol: th eEND\n",
      "New symbol: e n\n",
      "New symbol: o m\n",
      "New symbol: t oEND\n",
      "New symbol: , END\n",
      "New symbol: ! END\n",
      "New symbol: h a\n",
      "New symbol: e rEND\n",
      "New symbol: i t\n",
      "New symbol: : END\n",
      "New symbol: y ou\n",
      "New symbol: a l\n",
      "New symbol: o w\n",
      "New symbol: . .END\n",
      "New symbol: s t\n",
      "New symbol: k END\n",
      "New symbol: e r\n",
      "New symbol: i sEND\n",
      "New symbol: f END\n",
      "New symbol: on END\n",
      "New symbol: in END\n"
     ]
    },
    {
     "data": {
      "text/plain": "['\\\\ iEND',\n '/ i / END',\n 't o d a y - iEND',\n 'n ow iEND',\n '# you e v erEND',\n 'i f in al l yEND',\n '「 iEND',\n '- i - END',\n 'in e v aEND',\n '» iEND',\n 'w ha t t a y aEND',\n 'i i i i i i i i i iEND',\n '\\ue6d1 END',\n 'i k in d aEND',\n 'l o l - iEND',\n 'i a c t u al l yEND',\n 'w a d d y aEND',\n '# a s l on g a s you END',\n 'd o you END',\n '\\u200e \\u200b iEND',\n 'i ̇ END',\n 'ï END',\n '# l o l a t g i r l s w h oEND',\n '# r t i f you END',\n 'i j stEND',\n '« iEND',\n '• iEND',\n 'w h o d aEND',\n 'w ha d y aEND',\n ') iEND',\n '+ iEND',\n '# you r f a c e m a k e s m eEND',\n 'i i i i i i i iEND',\n '` iEND',\n 'i i i i i i iEND',\n 'i al re a d yEND',\n '_ iEND',\n '# you m a k e m eEND',\n '* iEND',\n '| iEND',\n '# u r b o y f r i en d e v erEND',\n 'w h en iEND',\n 'ι END',\n \"d on ' t c haEND\",\n \"w h o ' d aEND\",\n 'd you END',\n 'w ha d d a y aEND',\n 'i on l yEND',\n 'i j u s sEND',\n 'i al w a y sEND',\n 'i i i i iEND',\n 'd on c haEND',\n '( iEND',\n \"d ' y aEND\",\n 'ı END',\n '# u e v erEND',\n 'in e v erEND',\n 'i - iEND',\n 'i j u sEND',\n '/ / iEND',\n 'i st i l lEND',\n 'w ha d d y aEND',\n \"d ' you END\",\n 'i re al l yEND',\n 'd on t c haEND',\n 'i j u stEND',\n 'iEND',\n '- iEND',\n 'i you END',\n '# in n ow a y s ha p e o r f o r m END',\n '( you END',\n '/ / w eEND',\n '/ / u END',\n '# m en m a r r y w om en tha tEND',\n '/ w eEND',\n 's e l f - e d u c a t i onEND',\n '# re al g r an d m a sEND',\n '/ you END',\n '# s h ou t ou t t o g i r l s w h oEND',\n '# b o y s w h oEND',\n 'i / w eEND',\n '# s h ou t ou t t o th e g u y stha tEND',\n '/ / you END',\n '# i l o v e p e o p l e tha tEND',\n '# n o t al l b l a c k p e o p l eEND',\n '# i c an t st an d p e o p l e tha tEND',\n '# s h ou t ou t t o th e g i r l stha tEND',\n '- th e yEND',\n '- w eEND',\n '# h ow m an y p e o p l eEND',\n '- you END',\n 'w eEND',\n '# a q u a r i an sEND',\n 't th e yEND',\n 'th w yEND',\n 'g u i l d en st er n END',\n \"d ' u END\",\n '# i ha t e m al e s w h oEND',\n 't e h yEND',\n 'th r yEND',\n 'i f you END',\n '# h ou s e h i p p o sEND',\n 'th e u END',\n 'th e e yEND',\n '# i ha t e f e m al e s w h oEND',\n 'th e y yEND',\n 'th e yEND',\n 'v i o l e t sEND',\n 'e h oEND',\n 'w h o ’ dEND',\n 'w h o t fEND',\n 'w h o ’ v eEND',\n 'w h o dEND',\n '< U R L - re al . c om >END',\n '# i l i k e p e o p l e w h oEND',\n '- w h oEND',\n 'w h 0 END',\n 'w h u END',\n 'w h oEND',\n \"w h o ' v eEND\",\n 's s h eEND',\n 's er - u e b er w a c h erEND',\n 's h h eEND',\n 't e st a st er i s kEND',\n '# m y d u m b a s sEND',\n 's j eEND',\n 't a c h om a st erEND',\n 'i al m o stEND',\n 'i d on eEND',\n '# w ha t i f iEND',\n 'h e / s h e / itEND',\n '$ h eEND',\n '# w ha t i f g o dEND',\n '# i h e a r d c h u c k n o r r isEND',\n '# f m h 2 0 1 1 END',\n '# i h e a r d b ow w ow END',\n 'b l d _ 6 0 0 _ k w h END',\n 'b l d _ 6 5 0 _ k w h END',\n 's h e e eEND',\n '# f m 2 0 1 1 END',\n '- s h eEND',\n '- h eEND',\n 's / h eEND',\n 's h e / h eEND',\n '- itEND',\n 's h e eEND',\n 'h e / s h eEND',\n 'h eEND',\n 's h eEND',\n '< U R L - i . v e >END',\n \"l ' v eEND\",\n 'th e y v END',\n \"you \\\\ ' v eEND\",\n \"i ' b eEND\",\n '< U R L - i h e a r t m o v i e s . o r g >END',\n 'i w ou l d aEND',\n 'w e ` v eEND',\n \"i ha v en ' tEND\",\n '# i ha v en e v erEND',\n \"a : i ' v eEND\",\n 'w e v END',\n 'w e ´ v eEND',\n \"y u ' v eEND\",\n 'u ’ v eEND',\n \"- i ' v eEND\",\n \"th ere ' v eEND\",\n \"i ' d aEND\",\n '< U R L - v o om a x er . c om >END',\n \"tha t ' v eEND\",\n \"w e ' v END\",\n 'you ´ v eEND',\n 'i v e eEND',\n \"i ' d ' v eEND\",\n 'you ` v eEND',\n \"i \\\\ ' v eEND\",\n 'you v END',\n \"you ' v END\",\n '# n e v er ha v e i e v erEND',\n \"u ' v END\",\n '< U R L - n a u g h t y d o g . c om >END',\n 'i ha v en tEND',\n \"i v ' eEND\",\n 'th e y ’ v eEND',\n '# ha v e you e v erEND',\n 'i ´ v eEND',\n 'i ` v eEND',\n '# ha v e u e v erEND',\n 'th e y v eEND',\n \"i ' v END\",\n '0 . 0 0 % END',\n 'w e v eEND',\n 'u v eEND',\n 'w e ’ v eEND',\n \"u ' v eEND\",\n 'you ’ v eEND',\n 'you v eEND',\n 'i ’ v eEND',\n \"th e y ' v eEND\",\n \"i ' v eEND\",\n 'i v eEND',\n \"you ' v eEND\",\n \"w e ' v eEND\",\n 'n o o o o o o tEND',\n 'n o t t t t t t tEND',\n 'n o h tEND',\n 'you ha v eEND',\n '/ n o t / END',\n 'n o itEND',\n '- n o t - END',\n 'n o t t t t t tEND',\n 'n o o o o o tEND',\n \"n ' tEND\",\n 'n n o tEND',\n 'n a h tEND',\n '_ n o t _ END',\n 'd e s er v e d l yEND',\n 'n o t t t t tEND',\n 'n o o o o tEND',\n 'n o t - END',\n 'n o o o tEND',\n 'n o o tEND',\n 'n toEND',\n 'n o t t t tEND',\n 'n o t t tEND',\n 'r i g h t f u l l yEND',\n 'n a w tEND',\n 'n 0 tEND',\n 'n o t tEND',\n 'n o tEND',\n 'n tEND',\n 'g o t t n END',\n 'b 3 3 n END',\n 'b e e e e e en END',\n 'g o t t onEND',\n 's u c c e s s f u l yEND',\n 'b e e e e en END',\n 'b e en n END',\n 'u n d er g on eEND',\n 'b e e e en END',\n 'b e e en END',\n 'b e en END',\n 'g o t t en END',\n 'j u x tEND',\n '/ / j u stEND',\n 'j u st t t t tEND',\n 'j x tEND',\n '# s p o r c l eEND',\n 'j st tEND',\n 'j u r tEND',\n 'j y sEND',\n '/ j u stEND',\n '- j u sEND',\n 'j y stEND',\n 'd d e u b e lEND',\n 'j u st t t tEND',\n 'j u s s s stEND',\n 'j u $ tEND',\n 'j u u sEND',\n 'j s stEND',\n 'j u u u u u stEND',\n 'k u stEND',\n 'j h u sEND',\n 'j u s s s sEND',\n 'j h u s sEND',\n 'j u st sEND',\n 'j u s s stEND',\n 'j u s x END',\n 'j u x x END',\n 'j z tEND',\n 'j u z z END',\n 'j u d tEND',\n '< U R L - w o o . l y >END',\n 'j u h sEND',\n 'j u u u u stEND',\n 'j j u stEND',\n 'j u z tEND',\n 'j u st t tEND',\n 'j u st e dEND',\n 'j u s rEND',\n 'j u u u stEND',\n 'j u t sEND',\n 'j u s yEND',\n 'j u u stEND',\n 'j u a tEND',\n 'j u s z END',\n '# j u stEND',\n 'j s sEND',\n 'j u s s sEND',\n '< U R L - b u y t t er . c om >END',\n 'j u s stEND',\n 'j z END',\n '- j u stEND',\n '# d on t a c t l i k e you n e v erEND',\n 'j u tEND',\n 'j u st tEND',\n 'j u x END',\n 'j s u tEND',\n 'j u z END',\n 'j stEND',\n 'j u s sEND',\n 'j u stEND',\n 'j u sEND',\n \"a in \\\\ ' tEND\",\n 'a in ´ tEND',\n \"a ' in tEND\",\n 'a it n END',\n 'a in yEND',\n 'w u s z END',\n 'a y n tEND',\n 'a in n tEND',\n 'i an tEND',\n \"an ' tEND\",\n 'a in eEND',\n 'a in ` tEND',\n 'a in n END',\n 'a in t tEND',\n 'a i in tEND',\n 'i a in tEND',\n 'a in ’ tEND',\n 'an itEND',\n 'a inEND',\n 'a in tEND',\n \"a in ' tEND\",\n 's h ou d aEND',\n 's h ou l d n aEND',\n 's h ou l aEND',\n \"w ou l d n ' t ' v eEND\",\n \"s h ou l d n ' t ' v eEND\",\n 's h ou l d v END',\n \"s h u d ' v eEND\",\n 'han tEND',\n \"ha v ' n tEND\",\n 'c l d aEND',\n 'w l d v eEND',\n \"s h ou l d ' aEND\",\n 'w ou l d a aEND',\n 's h ou l d d aEND',\n 'w u l d v eEND',\n \"w u d ' v eEND\",\n 'w ou l d d aEND',\n 's h u l d v eEND',\n 's h ou l d a aEND',\n \"ha v e ' tEND\",\n 'c ou l d ’ v eEND',\n 'ha v en t tEND',\n 's h l d v eEND',\n 'c u d v eEND',\n \"m a y ' v eEND\",\n \"h v n ' tEND\",\n 'w ou l d ’ v eEND',\n 'a v n tEND',\n 'w l d aEND',\n 's h ou l d ’ v eEND',\n 'c u l d aEND',\n 'ha v en ´ tEND',\n 's h l d aEND',\n 'm i g h t v eEND',\n 'ha v en ` tEND',\n 'ha d n ’ tEND',\n '# g l o c al u r b an END',\n 'h v en tEND',\n 's h u d v eEND',\n 'w u d v eEND',\n \"ha v e ' n tEND\",\n 'c u d d aEND',\n 'm i g h t aEND',\n 'w u l d aEND',\n 's h u l d aEND',\n 'w u d d aEND',\n 's h u d d aEND',\n 'w u d aEND',\n 's h u d aEND',\n 'm u st v eEND',\n 'h v n tEND',\n \"m i g h t ' v eEND\",\n 'ha d n tEND',\n \"ha v n ' tEND\",\n 'ha v en ’ tEND',\n 'c ou l d v eEND',\n 'm u st aEND',\n \"m u st ' v eEND\",\n 'w ou l d v eEND',\n 's h ou l d v eEND',\n 'ha v n tEND',\n 'c ou l d aEND',\n \"c ou l d ' v eEND\",\n 'w ou l d aEND',\n \"ha d n ' tEND\",\n \"s h ou l d ' v eEND\",\n \"w ou l d ' v eEND\",\n 's h ou l d aEND',\n \"ha v en ' tEND\",\n 'ha v en tEND',\n 'n e v v aEND',\n 'n e e e e v erEND',\n 'n e v e tEND',\n 'n e e e v erEND',\n 'en v erEND',\n 'n er v erEND',\n 'n e e v erEND',\n 'n e v a a aEND',\n 'b e v erEND',\n '# in e v erEND',\n 'g l a d yEND',\n 'n e v e erEND',\n '- n e v erEND',\n \"n e ' erEND\",\n 'l e t c haEND',\n 'l e t c h u END',\n 'n e v er r r rEND',\n 'n v aEND',\n 'n e v a h END',\n 'n e v a aEND',\n 'n e v er r rEND',\n 'n v erEND',\n 'n e v er rEND',\n '# n e v erEND',\n 'n e v rEND',\n 'g l a d l yEND',\n 'n v rEND',\n 'n e v erEND',\n 'n e v aEND',\n 'e v u rEND',\n 'e v a a a a aEND',\n 'e v e aEND',\n 'e v e e e erEND',\n 'e v er r r r r r r r rEND',\n 'e v er r r r r r r rEND',\n 'e v e e erEND',\n 'e v a a a aEND',\n 'e v e erEND',\n 'n e v a rEND',\n 'e v er r r r r r rEND',\n 'e v a a aEND',\n 'e v a aEND',\n 'e v er r r r r rEND',\n 'e v er r r r rEND',\n 'e v a h END',\n 'e v er r r rEND',\n 'e v er rEND',\n 'e v er r rEND',\n 'e v rEND',\n 'e v a rEND',\n 'e v aEND',\n 'e v erEND',\n 'on l eEND',\n 'in l yEND',\n 'on l e eEND',\n 'on l u END',\n 'on y lEND',\n 'on l l yEND',\n 'on l tEND',\n 'on l y y yEND',\n 'o l n yEND',\n '- on l yEND',\n '0 n l yEND',\n 'on l i iEND',\n 'on yEND',\n 'on l y yEND',\n 'on l yEND',\n 'on l iEND',\n 'g e t 2 END',\n 'n e c c e s a r i l yEND',\n 'n e c c e s s a r i l yEND',\n 'e e e m END',\n 'e v er n END',\n 'n e v en END',\n 'l e t e m END',\n 'e v en n n END',\n 'e v e b END',\n 'e e en END',\n 'e v e m END',\n 'e v e en END',\n '< U R L - g t p 1 2 3 . c om >END',\n '- e v en END',\n \"1 0 x ' sEND\",\n \"m a k e ' e m END\",\n \"l e t ' e m END\",\n 'e v en n END',\n 'e e m END',\n 'e v n END',\n 'e v en END',\n 'n e c e s s a r i l yEND',\n 're e e e e e e al l yEND',\n 're a a al yEND',\n 're al l l l l l l l l l yEND',\n 'r l iEND',\n 're al l i eEND',\n 're al l l l l y y yEND',\n 're e l iEND',\n 'r 3 al l yEND',\n 're al l l l y yEND',\n 're al l y - re al l yEND',\n 're l al yEND',\n 're al l l l y y y yEND',\n 'r i l l iEND',\n 're al l y re al l y re al l yEND',\n '- re al l y - END',\n 're al l y y y y y yEND',\n 're a al l y yEND',\n 'e al l yEND',\n 're e e a a al l yEND',\n 're al l y re al l yEND',\n 're e a al l yEND',\n 'r re al l yEND',\n 're a a a al l l yEND',\n 're al l u END',\n 're a a a a a al l yEND',\n '/ re al l y / END',\n 're al y yEND',\n 're a al l l yEND',\n 're al l l l l l l l l yEND',\n 're al l l l y y yEND',\n 're a a al l l yEND',\n 'w e al l yEND',\n 're e e e e e al l yEND',\n 're al l l y y y yEND',\n 're l l iEND',\n 'g en u in l yEND',\n 're al l tEND',\n 're al l i iEND',\n 're al l l y yEND',\n 're a al yEND',\n 're al l l l l l l l yEND',\n '_ re al l y _ END',\n 're al l y y y y yEND',\n 're al l y 2 END',\n 's h o l eEND',\n 're a a a a al l yEND',\n 're e l yEND',\n 're l l eEND',\n 're al l l y y yEND',\n 's h o lEND',\n 're e al l yEND',\n 're e e e e al l yEND',\n 're al l l l l l l yEND',\n 'r i l l yEND',\n 're al l y y y yEND',\n 're a a a al l yEND',\n 're a a al l yEND',\n 'r i l iEND',\n 're e e al l yEND',\n 're a al l yEND',\n 're al l l l l l yEND',\n 're e e e al l yEND',\n 're al l y y yEND',\n 'r i l yEND',\n 's h o l lEND',\n 're al iEND',\n 're l iEND',\n 're al l l l l yEND',\n 're l l yEND',\n 're al l iEND',\n 're l eEND',\n 're al l y yEND',\n 're al l l l yEND',\n 're al l l yEND',\n 'r l l yEND',\n 'g en u in e l yEND',\n 're al yEND',\n 're al l yEND',\n 'r l yEND',\n 'al re d a yEND',\n 'al re a y dEND',\n 'f in al iEND',\n 'o f f i s h END',\n 'al r a d yEND',\n 'w o o d aEND',\n 'o re d iEND',\n 'al re a a d yEND',\n 'al re a d y y y y yEND',\n 'al re a d d yEND',\n 'al e a d yEND',\n 'al re a yEND',\n 's u c e s s f u l l yEND',\n 'al re d iEND',\n 'a re a d yEND',\n 'al re a d iEND',\n 'al re a d i iEND',\n 'al re a dEND',\n 'al re a d y y yEND',\n 'a w re a d yEND',\n 'al r dEND',\n 'c u d aEND',\n 'al re d yEND',\n 'al l re a d yEND',\n 'al re a d y yEND',\n 'al r d yEND',\n 'p re v i ou s l yEND',\n 'al re a d yEND',\n 're c en t l yEND',\n '- al m o stEND',\n 'n e al yEND',\n 'n e a r l l yEND',\n 'al m 0 stEND',\n 'al om o stEND',\n 'al om stEND',\n 'al m o st tEND',\n 'al m s o tEND',\n 'a m o stEND',\n 'al l m o stEND',\n 'al m stEND',\n 'a v er a g ingEND',\n 'r ou g h l yEND',\n 'v i r t u al l yEND',\n 'a p p r o x i m a t e l yEND',\n 'p r a c t i c al l yEND',\n 'al m o stEND',\n 'n e a r l yEND',\n 's i b b yEND',\n 'c u r ren t yEND',\n 'o f f i c a i l l yEND',\n 'o f f c i al l yEND',\n '# b g g p l a yEND',\n 'c u ren t l yEND',\n 'b u s i l yEND',\n 'o f f i c al yEND',\n 'o f i c i al l yEND',\n 'c o r d i al l yEND',\n '< U R L - l i st en . g h e t t o r a d i o . f m >END',\n '< U R L - k a i s ere g g . c h >END',\n 'h u c k l e b er r i e sEND',\n 'h e i s eEND',\n '# re a d c a stEND',\n 'p re s en t l yEND',\n 'o f f i c i al yEND',\n '< U R L - g o . n i k e . c om >END',\n 'o f f i c al l yEND',\n 're p o r t e d l yEND',\n 'o f f i c i al l yEND',\n 'c u r ren t l yEND',\n 'f in al l l y yEND',\n 'f in al l iEND',\n 'f in al l l y y yEND',\n '# o f f i c i al l yEND',\n 'f i i i in al l yEND',\n 'f n al l yEND',\n 'f in al l y y y y yEND',\n 'f i in al l yEND',\n '- f in al l yEND',\n 'b er l yEND',\n 'f in al l l l l l yEND',\n '# th in g s i d i d o v er th e s u m m erEND',\n '< U R L - m a p m y f it n e s s . c om >END',\n 'f in al l l l l yEND',\n 'f in i al l yEND',\n 's n a c k f e e dEND',\n 'f in al l y y y yEND',\n 'f in al l l l yEND',\n 'f i an l l yEND',\n '< U R L - m a p m y r i d e . c om >END',\n 'f in al l y y yEND',\n 's u c c e s f u l l yEND',\n 'f in al l y yEND',\n '# m y f it n e s s p alEND',\n 'f in n al l yEND',\n '< U R L - m a p m y r u n . c om >END',\n 'f in al l l yEND',\n 're l u c t an t l yEND',\n 'f in n al yEND',\n '< U R L - c o o r d . in f o >END',\n 'f in al yEND',\n 'f in al l yEND',\n 's u c c e s s f u l l yEND',\n 'k n ow e stEND',\n 'r a th rEND',\n 'c an stEND',\n '< U R L - s u p er m a r k e t . c om >END',\n 'r a thaEND',\n 'r a th erEND',\n 's hal tEND',\n \"d on t ' c haEND\",\n 'i i onEND',\n 'i m m oEND',\n '4 + 4 END',\n 'i i b END',\n 'i d on t tEND',\n 'b r in j alEND',\n 'i d onEND',\n 'th e l lEND',\n '2 iEND',\n 'n u stEND',\n 'a b t aEND',\n 'l e y sEND',\n 'm e + you END',\n 'l e m m aEND',\n 's h ou l d sEND',\n 'w e l l iEND',\n 'i / i iEND',\n 'd i d stEND',\n '1 iEND',\n 'c ha r s e tEND',\n 'u l dEND',\n 'd / n END',\n 'í END',\n 'f . i . n . a . l . s .END',\n 'i ow n END',\n 'i i dEND',\n 'n on - v i r g inEND',\n 's k y ha w kEND',\n 's k y l an eEND',\n 'c h _ t y p eEND',\n 'k en o tEND',\n 'd in n yEND',\n '# i d on tEND',\n '- i iEND',\n '# y a m a m a e v erEND',\n '2 0 1 0 / 0 7 END',\n '2 0 1 0 / 0 5 END',\n 'c . l . a . s . s .END',\n '& iEND',\n 'i f u END',\n 'f m tEND',\n 'tha t iEND',\n 'l e m iEND',\n '# m y g o al f o r 2 0 1 2 END',\n 'y u dEND',\n 'e b u END',\n 'b o t t aEND',\n 'm on e y - b a c kEND',\n '1 9 tEND',\n 'i 8 END',\n '# on l y f a t p e o p l eEND',\n '# th in g s i a in t d on e y e tEND',\n 'd on t c h u END',\n 's g eEND',\n 'i f iEND',\n 'i l dEND',\n '# c on f u s in g th in g s g i r l s d oEND',\n '_ i _ END',\n 'm u z END',\n 'c an iEND',\n '# u r g i r l f r i en d e v erEND',\n 'i - i - iEND',\n '# on l y u g l y p e o p l eEND',\n '2 0 1 0 / 1 2 END',\n 'l e t t sEND',\n 'n e erEND',\n '2 0 1 0 / 1 0 END',\n '2 0 1 0 / 0 4 END',\n 'c y a aEND',\n 's on tEND',\n '2 0 1 0 / 0 8 END',\n 'd on n END',\n '2 0 1 0 / 0 6 END',\n 'a p t - g e tEND',\n 'u onEND',\n 'd o an END',\n 'd o stEND',\n '# on l y w h it e p e o p l eEND',\n 'i i i iEND',\n '# th in g s b l a c k p e o p l e d oEND',\n 'i d w END',\n '2 + 2 END',\n 'in eEND',\n 'ha stEND',\n 'i d i d n tEND',\n '1 + 1 END',\n 'h e dEND',\n 'p r o v o k ingEND',\n 'i d n tEND',\n 'u lEND',\n 'u m aEND',\n 'w dEND',\n 'u dEND',\n 'i i iEND',\n 'l lEND',\n 'i v END',\n 'i onEND',\n 'i iEND',\n 'i dEND',\n 'p r a c t i c al yEND',\n 'u s s u al l yEND',\n 'a c c i d en t i al l yEND',\n 's u d en l yEND',\n 't e a r f u l l yEND',\n 'u s u al l l yEND',\n 'n a i v e l yEND',\n 're f l e x i v e l yEND',\n 'o p t i on al l yEND',\n 'l i k e 2 END',\n 'c on t in u o s l yEND',\n 'u n w i s e l yEND',\n 'p r a c t i c l yEND',\n 'a t u al l yEND',\n 'a c t l yEND',\n 'p u r p o s l yEND',\n 's e c re t e l yEND',\n 'ha r d l e yEND',\n 'a u t om a t i c al yEND',\n 'c on c e i v a b l yEND',\n 'a b s en t m in d e d l yEND',\n 'a c t u al l iEND',\n 'l it r al l yEND',\n 's u b c on c i ou s l yEND',\n 'c on st an l yEND',\n 'l it er l yEND',\n 'd o in tEND',\n 'm a k e m END',\n 's u p p o s e l yEND',\n 'a c t u al l y yEND',\n 'a c t u l yEND',\n 'g o an n aEND',\n 'b a s c i al l yEND',\n 'p r a t i c al l yEND',\n 'i i v eEND',\n 's u p p o s a b l yEND',\n 'b a s i c al yEND',\n 'er r on e ou s l yEND',\n 'f l a t l yEND',\n 'c a s j END',\n 'l e g it l yEND',\n 'l it t er l yEND',\n 'm u s s yEND',\n 'o r g in al l yEND',\n 'd on rEND',\n 'in a d v er t an t l yEND',\n 'a c t al l yEND',\n 'a c t u al iEND',\n 'u s u s al l yEND',\n 'a c c u al l yEND',\n 'in t u it i v e l yEND',\n 'a u t om a t i c l yEND',\n 'g r u d g in g l yEND',\n 'b e g r u d g in g l yEND',\n 's c a r c e l yEND',\n 'b a r l yEND',\n 'l it t er al yEND',\n 'b l a t en t l yEND',\n 'ha b it u al l yEND',\n 'o r d in a r i l yEND',\n 'a f f e c t i on a t e l yEND',\n 's n e a k i l yEND',\n 'a c c i d en t al yEND',\n 'n o r m al yEND',\n 's in g l e han d e d l yEND',\n 'c om p u l s i v e l yEND',\n 's u b l i m in al l yEND',\n 'in v o l u n t a r i l yEND',\n 'a c t u al l l yEND',\n 'g in eEND',\n 'd e f in it i v e l yEND',\n 'u s u al yEND',\n 'l it er al yEND',\n 'b r a v e l yEND',\n 'a c c t u al l yEND',\n 'a c u al l yEND',\n 'b a s i c l yEND',\n 'ha f fEND',\n 'in st in c t i v e l yEND',\n 'u n c on s c i ou s l yEND',\n 's in g l e - han d e d l yEND',\n 's l y l yEND',\n 'a c t a u l l yEND',\n 'd r u n k en l yEND',\n 'a c u t al l yEND',\n 'f o o l i s h l yEND',\n 'b e a r l yEND',\n 'p u r p o s e f u l l yEND',\n 'j o k in g l yEND',\n 'r ou t in e l yEND',\n 'k n ow in g l yEND',\n 's u b c on s c i ou s l yEND',\n 'u n k n ow in g l yEND',\n 'a c t u l l yEND',\n 'm i r a c u l ou s l yEND',\n 'l it t er al l yEND',\n 'c o l l e c t i v e l yEND',\n 't r a d it i on al l yEND',\n 'in a d v er t en t l yEND',\n 'm i st a k en l yEND',\n 'v o l u n t a r i l yEND',\n 'b l in d l yEND',\n 'in d i re c t l yEND',\n 's p on t an e ou s l yEND',\n 'w i l l in g l yEND',\n 'h ere b yEND',\n 'g r a d u al l yEND',\n 'a c t u al yEND',\n 'j e sEND',\n 'd e l i b er a t e l yEND',\n 'c on t in u ou s l yEND',\n 's e l d om END',\n 'in t en t i on al l yEND',\n 'p u r p o s e l yEND',\n 'b a r l e yEND',\n 'in it i al l yEND',\n 'a c t i v e l yEND',\n 'on tEND',\n 'c a s u al l yEND',\n 'e s s en t i al l yEND',\n 'p r ou d l yEND',\n 't y p i c al l yEND',\n 'a c c i d en t l yEND',\n 'm a g i c al l yEND',\n 'al l e g e d l yEND',\n 's u p p o s e d l yEND',\n 'o r i g in al l yEND',\n 's e c re t l yEND',\n 'g en er al l yEND',\n 'r a re l yEND',\n 'a u t om a t i c al l yEND',\n 'a c c i d en t al l yEND',\n 'r an d om l yEND',\n 'n o r m al l yEND',\n 'c on st an t l yEND',\n 'ha r d l yEND',\n 'b a re l yEND',\n 'b a s i c al l yEND',\n 'l it er al l yEND',\n 'a c t u al l yEND',\n 'u s u al l yEND',\n 's u d d en t l yEND',\n '- al w a y sEND',\n 'al o sEND',\n 'c o in c i d en t l yEND',\n 'p r a y er f u l l yEND',\n 'd e m on b r u en END',\n 'd e s p a r a t e l yEND',\n 'g i e sEND',\n 'd e s p er a t l yEND',\n 'a s l oEND',\n 'al t er n a t e l yEND',\n 'b e l a t e d l yEND',\n 'j x END',\n 's u b s e q u en t l yEND',\n 's in c er l yEND',\n 'h en c e f o r th END',\n 'u l t i m a t e l yEND',\n 'd e s p er a t e l yEND',\n 's in c ere l yEND',\n 's u d d en l yEND',\n 'al s oEND',\n 'o b i ou s l yEND',\n 'd e a d a z z END',\n 'e v i d en t al l yEND',\n 't o t al l y y yEND',\n 's r i ou s l yEND',\n 'o b v i ou l s yEND',\n 'o b v s l yEND',\n 's er o i u s l yEND',\n 'n e v er r r r r r rEND',\n 's er i s ou l yEND',\n 'h on e st l y yEND',\n 's r l s yEND',\n 's er i ou s l y y y yEND',\n 'l it er al l l yEND',\n 'o b v i o s l yEND',\n 'o v i ou s l yEND',\n 'g o t t c haEND',\n 'o b v z END',\n 'd / aEND',\n 'w i s h iEND',\n 'g o t c ha aEND',\n 'i i i i i i i i iEND',\n '# j u st c a u s e w e c o o lEND',\n \"s o r r y ' sEND\",\n 'l i k e e e e e eEND',\n 's h e e e eEND',\n 'g o t c h y aEND',\n 's r l yEND',\n 'f er re alEND',\n 's er i ou s l y y yEND',\n 's i r i u s l yEND',\n 'g e z END',\n 'n e v er r r r r rEND',\n 'l o k e yEND',\n 's u r l e yEND',\n 's er i ou s l l yEND',\n 'h on e s l t yEND',\n 's er i u o s l yEND',\n 'd e a d a s s sEND',\n 'l ow k e y yEND',\n '- re al l yEND',\n 's er i u s l yEND',\n 's er i ou l yEND',\n 'b e t c h u END',\n 's er ou s l yEND',\n 's er z l yEND',\n 'h i g h k e yEND',\n 'n e v er r r r rEND',\n 's er i o s l yEND',\n 'l i k e e e e eEND',\n 's er i ou l s yEND',\n 's er i o s u l yEND',\n 'p er s on al yEND',\n 'u n d er st an d a b l yEND',\n 'u n n oEND',\n 's er i ou s l y yEND',\n 'g e d d itEND',\n 'l i k e e e eEND',\n 'th e o re t i c al l yEND',\n 'd . aEND',\n 're al i st i c al l yEND',\n 'o b v iEND',\n 't r u th f u l l yEND',\n 'o b v sEND',\n 'b e t c haEND',\n '# l ow k e yEND',\n 'o b v END',\n ...]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preform_bpe(brown_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jakie angielskie słowo jako pierwsze dostało swój własny token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df4c7b8b5aa2b077eaa2d42429797139",
     "grade": true,
     "grade_id": "cell-acd48c77e2c1bcec",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Najszybciej swój token dostało słowo \"in\", jeżeli jednak nie liczymy go jako słowa będzie to następne w kolejce \"MENTION\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jakie są zalety korzystania z tokenizacji BPE w kontekście tworzenia reprezentacji (problem OOV, odnieś się do  k-gramów i n-gramów)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64306e36b58f1eee12c8bb339123e105",
     "grade": true,
     "grade_id": "cell-006ef6fd3e397206",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Zaletą korzystania z BPE w kontekście problemu OOV jest z pewnością fakt, że tokenizujemy najczęściej występujące pary symboli a nie całe słowa, co pozwala rozwiązać problem literówek. Kolejną zaletą możę być to, że potrzebujemy mniejszej wiedzy na temat danego języka w przeciwieństwie do n-gramów, ponieważ tutaj tworzenie tokenów odbywa się od podstaw automatycznie. Odnosząc się natomiast to k-gramów, reprezentacja BPE jest znacznie bardziej efektywne (wymaga mniejszej ilości cech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wróć do implementacji i zakomentuj wypisywanie (funkcje print) informacji z funkcji `preform_bpe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - klasyfikacja (15 pkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod powinien wczytać i ztokenizować zbiór danych dot. analizy wydźwięku. Jeśli nie masz biblioteki `nltk` musisz ją zainstalować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data set ['tweets.txt']\n"
     ]
    }
   ],
   "source": [
    "from helpers import DataSet\n",
    "\n",
    "training_set = DataSet(['tweets.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej znajdziesz przykład odczytu jednego tweeta z obiektu DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\n",
      "['dear', '@microsoft', 'the', 'newooffice', 'for', 'mac', 'is', 'great', 'and', 'all', ',', 'but', 'no', 'lync', 'update', '?', \"c'mon\", '.']\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "for i in training_set.tweets:\n",
    "    print(i.text)\n",
    "    print(i.tokens)\n",
    "    print(i.clazz)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemy IL często pracują z bardzo dużą liczbą cech, które są rzadkie np. cechy Bag-Of-Words, cechy n-gramowe itd. Powoduje to że klasyczna macierz przykłady uczące na cechy rośnie do bardzo dużych rozmiarów nawet dla małych zbiorów uczących (w sensie liczby przykładów). Ponadto samo przechowywanie w pamięci słownika mapującego konkretne słowa/n-gramy na indeksy kolumn macierzy może być bardzo kosztowne pamięciowo przy dużych rozmiarach słownika.\n",
    "\n",
    "Istnieje jednak technika, która pozwala nam na ominięcie tej przeszkody: haszowanie cech. Opis tej techniki znajdziesz na stronie:  https://en.wikipedia.org/wiki/Feature_hashing Jest ona też implementowana w obiekcie `sklearn.feature_extraction.FeatureHasher`. Zapoznaj się z opisem techniki i wykonaj poniższe polecenia.\n",
    "\n",
    "- Wykorzystując haszowanie cech wytrenuj wybrany klasyfikator (najlepiej taki, który się szybko liczy) na zbiorze uczącym dla cech Bag-of-words (możesz też spróbować cechy n-gramowe). Możesz wykorzystać gotową tokenizację we właściwości `.tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac05ad71ee90b1c800030849c5321cb7",
     "grade": true,
     "grade_id": "cell-f6cfe39258fbec51",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5216426193118757\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import re\n",
    "\n",
    "\n",
    "def process_data(data, n_features):\n",
    "    tokens_per_text = []\n",
    "\n",
    "    c = Counter()\n",
    "    for tokens in [word_tokenize(obj.text.lower()) for obj in data]:\n",
    "        c.update(tokens)\n",
    "        tokens_per_text.append(tokens)\n",
    "\n",
    "    for k in list(c.keys()):\n",
    "        if re.findall(\"\\W\", k) != [] or k in stopwords.words('english'):\n",
    "            c.pop(k)\n",
    "\n",
    "    most_common = [x[0] for x in c.most_common(n_features)]\n",
    "\n",
    "    data_out = []\n",
    "    for tokens in tokens_per_text:\n",
    "        cc = Counter()\n",
    "        cc.update(tokens)\n",
    "\n",
    "        for key in list(cc.keys()):\n",
    "            if key not in most_common:\n",
    "                cc.pop(key)\n",
    "\n",
    "        data_out.append(dict(cc))\n",
    "\n",
    "    return FeatureHasher(n_features=n_features).fit_transform(data_out, y).toarray()\n",
    "\n",
    "\n",
    "n_features = 100\n",
    "y = np.array([y.clazz for y in training_set.tweets])\n",
    "X = process_data(training_set.tweets, n_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stwórz wykres zależności wybranej miary klasyfikacji od wymiarów macierzy danych (chodzi o liczbę cech do których haszujemy cechy oryginalne). Wystarczy przetestować kilka (>=4) wybranych wartości na skali logarytmicznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd253bac561b269cff3a3dceadc70f0",
     "grade": true,
     "grade_id": "cell-8076c16242981ae9",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = []\n",
    "n_features = range(10, 200, 5)\n",
    "\n",
    "for i in n_features:\n",
    "    y = np.array([y.clazz for y in training_set.tweets])\n",
    "    X = process_data(training_set.tweets, i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.plot(list(n_features), scores)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Obserwując stworzony wykres - skomentuj. Jak dużo jakości klasyfikacji się traci (albo zyskuje?) korzystając z mniejszej liczby haszowanych cech? Często klasyfikatory bardzo dobrze działają nawet przy liczbie haszowanych cech dla których na pewno istnieją konflikty cech oryginalnych - jak myślisz dlaczego? (Pomyśl o interpretacji takich skonfliktowanych cech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed30f2d487da41cf1a92ffb63195d621",
     "grade": true,
     "grade_id": "cell-2caea1821af5d8aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Generalny wynik klasyfikacji należy zaliczyć do słabych, ponieważ maksymalna trafność wyniosła ok. 55% (co jest wynikiem niewiele lepszym od algorytmu losowego). Wraz ze wzrostem ilości cech zyskujemy również poprawę jakości klasyfikacji (której jakość ciężko ocenić ze względu niski wynik od samego początku). Skonfliktowane cechy nie powodują spadku jakości klasyfikacji, ponieważ kolizje są powodowane przez to że podobne symbole mają podobne wartości funkcji haszowej, a jeżeli są podobne do siebie to można również podejrzewać że są podobne znaczeniowo i mają bliskoznaczne znaczenie dla danego tekstu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20139da166319b49eea5cc7e984fc08e",
     "grade": false,
     "grade_id": "cell-0d86672dbabbf54d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    " - W poprzednim zadaniu wczytałeś wynik grupowania Browna do pamięci. Wytrenuj klasyfikator na reprezentacji ,,Bag-of-clusters'' tj. w kolumnach zamiast słów/n-gramów będziesz miał grupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52330743618202\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "c = Counter()\n",
    "for row in training_set.tweets:\n",
    "    c.update(row.tokens)\n",
    "\n",
    "tweets_df = DataFrame.from_dict({'word': c.keys(), 'count': c.values()})\n",
    "data = preform_bpe(tweets_df, 20)\n",
    "\n",
    "feature_names = set()\n",
    "for x in data:\n",
    "    feature_names.update(set(x.split(' ')))\n",
    "\n",
    "y = np.array([y.clazz for y in training_set.tweets])\n",
    "X = pd.DataFrame(columns=feature_names, data=np.zeros(shape=(y.shape[0], len(feature_names)), dtype=int))\n",
    "for i in range(y.shape[0]):\n",
    "    for fn in feature_names:\n",
    "        if training_set.tweets[i].text.find(fn) != -1:\n",
    "            X.iloc[i][fn] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Podsumuj eksperymenty: poznałeś dwie możliwości ograniczenia liczby cech - zastąpienie słów ich grupami i haszowanie cech. Jakie są wady i zalety obydwu podejść?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80ace505afba9b12fd5d3896a9046ef",
     "grade": true,
     "grade_id": "cell-4508400659f7243e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Uzyskanie rezultaty klasyfikacji dla obu metod są równie złe dla obu metod, zapewne jest to wina implementacji i jakichś błędów których nie dostrzegam, dlatego na pytania odpowiem teoretycznie. Zaletą FeatureHashera jest to, że można precyzyjnie ustalić ilość cech i na tej podstawie dobrać ten parametr dla jak najlepszego wyniku klasyfikacji. Nie można tego samego powiedzieć o BPE, dla którego trzeba dobrać odpowiednią ilość iteracji. Zaletą BPE pozostaje w dalszym ciągu automatyzacja przetwarzania, co uniezależnia go niejako o problemu w generowaniu poprawnych tokenów. W przypadku FH wiele zależy od tokenizacji."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}