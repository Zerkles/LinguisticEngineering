{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zaawansowane Przetwarzanie Języka Naturalnego\n",
    "## Laboratorium 3: Neuronowe modele predykcji sekwencji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Celem zadania jest zaimplementowanie neuronowego modelu do predykcji sekwencji i zastosowanie go do zadania płytkiej analizy frazowej. Zaimplementowany model będzie oparty o dwukierunkowe sieci rekurencyjne oraz zakończony będzie warstwą CRF. Celem ćwiczenia nie jest stworzenie wydajnej implementacji odpowiedniej do pracy z dużymi zbiorami treningowymi, ale pogłębienie zrozumienia działania metod modelowania predykcji poprzez ich implementację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie 1\n",
    "Pracując w środowisku PyTorch nie jest konieczne samodzielne implementowanie neuronów rekurencyjnych, gdyż biblioteka `torch.nn` zawiera ich gotowe implementacje. Klasyczny neuron rekurencyjny z funkcją aktywacji tangens hiperboliczny jest zaimplementowany w klasie `RNN` i od razu pozwala na przetworzenie od razu całej sekwencji elementów. Konstruktor obiektu wymaga wyspecyfikowania liczby cech którymi opisany jest każdy element sekwencji, liczby neuronów w warstwie rekurenycjnej oraz liczbę warstw rekurencyjnych. \n",
    "\n",
    "Istotnym parametrem modelu jest opcjonalny przełącznik `batch_first = True` który określa format wejściowej sekwencji. Domyślnie modele rekurencyjne spodziewają się wejścia o wymiarach `(długość sekwencji, rozmiar batcha, liczba cech)` czyli w kolumnach tensora mamy kolejne sekwencje (przykłady uczące), w wierszach kolejne elementy tych sekwencji, a w głębokości tensora umieszone są kolejne cechy każdego z elementów sekwencji. Zwróć uwagę, że jest to format inny od często stosowanego formatu danych niesekwencyjnych gdzie w wierszach umieszcza się kolejne przykłady uczące, a w kolumnach kolejne cechy (tutaj jest na odwrót: przykład uczący-sekwencja umieszcona jest w kolumnie, a nie w wierszu). Jeśli jednak ustawisz przełącznik `batch_first = True` to sieć rekurencyjna będzie się spodziewała wejścia jako `(rozmiar batcha, długość sekwencji, liczba cech)` czyli kolejne sekwencje będą w kolejnych wierszach tensora.\n",
    "\n",
    "Dalej, warto zauważyć, że w przypadku np. warstwy liniowej do obliczenia wyniku wystarczyło podanie samych danych ( `linear(dane)`), jednak wejściem sieci rekurencyjnej są nie tylko dane ale i poprzednie stany ukryte $h_{t-1}$ ( `rnn(dane, h0)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "N_FEATURES = 2\n",
    "N_LAYERS = 1\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "data = torch.FloatTensor([[1, 1], [2, 2], [3, 3], [4, 4]]).view(1, 4, 2)\n",
    "# Proste przykładowe dane: jedna 4 elementowa sekwencja gdzie każdy jej element jest opisany dwoma cechami\n",
    "# Zwróć uwagę na wymiary tensora (rozmiar batcha, długość sekwencji, liczba cech elementów) \n",
    "#     - konieczne batch_first=True\n",
    "\n",
    "rnn = nn.RNN(N_FEATURES, HIDDEN_SIZE, N_LAYERS, batch_first=True)\n",
    "\n",
    "h0 = torch.randn(N_LAYERS, 1, HIDDEN_SIZE)\n",
    "# Początkowy stan ukryty h0. Każda warstwa sieci rekurencyjnej zaczyna przetwarzanie od swojego h0\n",
    "# W sytuacji gdy przetwarzamy kilka sekwencji na raz (sieć jest uruchamiana równolegle na kilku sekwencjach)\n",
    "# również potrzebujemy dla każdej przetwarzanej sekwencji jej stan początkowy h0\n",
    "# Porównaj powyższy opis z wymiarowością zmiennej h0\n",
    "\n",
    "out, h_t = rnn(data, h0)\n",
    "print(\"Wyjście warstwy: \", out)\n",
    "print(\"Ostatni stan ukryty: \", h_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zwróć uwagę na otrzymane wyjście: ponieważ na wejście sieci podaliśmy sekwencję elementów to dla każdego przetwarzanego elementu sekwencji sieć zwróciła uzyskaną reprezentację ukrytą (skoro mamy 3 neurony w warstwie to taka reprezentacja jest 3-wymiarowa). Dodatkowo zwrócony został ostatni stan ukryty, który moglibyśmy przetwarzać np. kolejnej sieci rekurencyjnej.\n",
    "\n",
    "Sprawdź poprawność uzyskanego powyżej wyniku poprzez uruchomienie sieci rekurencyjnej element po elemencie (wejściem do sieci jest zawsze tylko jeden element sekwencji lub inaczej: sekwencje 1-elementowe). Wypisz na wyjście kolejne otrzymywane wyniki sieci i jej stany ukryte. Uzyskane wyniki powinny być takie same jak te z komórki wyżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h1 = h0\n",
    "print(h0)\n",
    "for i in range(1, 5):\n",
    "    x_i = torch.FloatTensor([[i, i]]).view(1, 1, 2)\n",
    "    out, h1 = rnn(x_i, h1)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "W prosty sposób można również rozszerzyć naszą warstwę rekurencyjną do sieci dwukierunkowej. W konstruktorze wystarczy ustawić `bidirectional = True`. Jak mówiliśmy na wykładzie taka sieć składa się w rzeczywistości z dwóch sieci - jednej przetwarzającej wyniki od lewej do prawej i drugiej przetwarzającej wyniki od prawej do lewej. W związku z tym należy dla obydwu tych sieci przygotować ich stany początkowe $h_0$, a wymiarowość wyjścia zwiększy się dwukrotnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rnn = nn.RNN(N_FEATURES, HIDDEN_SIZE, N_LAYERS, batch_first=True, bidirectional=True)\n",
    "\n",
    "h0 = torch.randn(N_LAYERS * 2, 1, HIDDEN_SIZE)\n",
    "out, h_t = rnn(data, h0)\n",
    "print(\"Wyjście warstwy: \", out)\n",
    "print(\"Ostatni stan ukryty: \", h_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Analogicznie jak poprzednio uzyskaj wynik z poprzedniej komórki zakładając że na wejściu obiektu `rnn` możesz umieścić jedynie sekwencje jednoelementowe (możesz go jednak wywoływać dowolnie wiele razy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h1 = h0\n",
    "for i in range(1, 5):\n",
    "    x_i = torch.FloatTensor([[i, i]]).view(1, 1, 2)\n",
    "    out, h1 = rnn(x_i, h1)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na koniec warto też zauważyć, że samodzielna inicjalizacja stanu wejściowego do sieci rekurencyjnej `h0` nie jest obowiązkowa i na wejście warstwy rekurencyjnej można podać samą sekwencję wejściową. W takim wypadku element `h0` zostanie zainicjalizowany domyślnie wektorem zer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out, h_t = rnn(data)\n",
    "print(\"Wyjście warstwy: \", out)\n",
    "print(\"Ostatni stan ukryty: \", h_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Ćwiczenia**\n",
    "- Jakich zmian w kodzie musiałbyś dokonać by uzyskać 3-warstwową sieć rekurencyjną? Jak zmieniłaby się wtedy wymiarowośc `h0` oraz wyjścia sieci neuronowej `h1` i `out`?\n",
    "- W przypadku neuronu rekurencyjnego RNN wejściem do modelu jest sekwencja oraz początkowy/poprzedni stan ukryty `h0`. Korzystając z gotowej implemetancji neuronu LSTM jakiego wejścia się spodziewasz?\n",
    "\n",
    "Odpowiedzi nie musisz zapisywać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie 2\n",
    "W ostatnim zadaniu domowym poznawałeś usprawnienia implementacji modeli uczących się w PyTorch poprzez wykorzystanie gotowych elementów z modułu `torch.nn`. Jednak ostatecznie uzyskany przez nas kod nadal przetwarzał pojedyncze instancje tj. nieobsługiwał mini-batchy. Można oczywiście ręcznie zaimplementować indeksowanie, które pozwoli nam na iterowanie po paczkach danych, jednak PyTorch pozwala oczywiście na automatyzację tego procesu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na początek wczytajmy dane uczące z których będziemy korzystać w tym zadaniu. (Może być konieczna instalacja biblioteki `torchtext`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import CoNLL2000Chunking\n",
    "\n",
    "train_iter = CoNLL2000Chunking(split='train')\n",
    "seq_texts = []\n",
    "seq_tags = []\n",
    "for i in train_iter:\n",
    "    seq_texts.append(i[0])\n",
    "    seq_tags.append(i[2])\n",
    "print(seq_texts[0])\n",
    "print(seq_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Są to dane z popularnego zbioru CoNLL2000 dotyczące zadania płytkiej analizy frazowej. W liście `seq_texts` umieszczono kolejne zdania, a na liście `seq_tags` umieszczono odpowiadające im zestawy tagów. Zwróć uwagę, że zastosowano tutaj tagowanie BIO np. frazą czasownikową `VP` jest \"is widely expected to take\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pierwszym krokiem przetwarzania jest zamiana słów na ich identyfikatory oraz obsłużenie tokenu `OOV`. W poprzednich zadaniach domowych robiliśmy to ręcznie jednak dzięki bibliotece `torchtext` ten proces równiez możemy zautomatyzować dzięki obiektowi `Vocab` i funkcji `build_vocab_from_iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab = build_vocab_from_iterator(seq_texts, specials=[\"<unk>\"], min_freq=5)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Funkcja ta na wejście przyjmuje iterator z tekstami, a także opcjonalnie próg `min_freq` pomijający słowa występujące z niewystarczającą częstotliowścią oraz `specials` pozwalający na umieszczenie w słowniku dodatkowych tokenów. Tokeny specjalne domyślnie są umieszczane na początku słownika.\n",
    "\n",
    "Obiekt `Vocab` w prosty sposób zamienia sekwencję tokenów na sekwencję ich indeksów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab([\"if\", \"could\", \"in\", \"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "oraz na inne proste konwersje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(vocab.lookup_token(107))\n",
    "print(vocab.lookup_indices([\"<unk>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Domyślnie jednak słownik nie obsługuje słów spoza słownika. Dla przykładu \"Confidence\" występuje w rozważanym korpusie mniej niż 2 razy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#vocab([\"Confidence\"]) #TODO: Uncomment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Można jednak ustawić domyślny indeks słownika na token `UNK`, który automatycznie zamienia nieznane słowa na indeks tego tokenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab.set_default_index(0)\n",
    "vocab([\"if\", \"could\", \"in\", \"<unk>\", \"Confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Następny krokiem jest implementacja własnej klasy zbioru danych, dziedziczącej po klasie `torch.utils.data.Dataset`, która zapewni kompatybilność reprezentacji naszych danych z procedurami m.in. automatycznie tworzącymi batche.\n",
    "\n",
    "Zbiór danych powinien implementować co najmniej 3 funkcje: konstruktor, funkcję zwracającą liczbę elementów w zbiorze `__len__` oraz funckję pozwalającą na dostęp do wybranego elementu zbioru `__getitem__`.\n",
    "\n",
    "Wykorzystując funkcję `build_vocab_from_iterator` uzupełnij implementację poniższej klasy, tak aby funkcja `__getitem__` zwracała elementy zbioru składające się z sekwencji indeksów słów oraz z sekwencji indeksów klas. Należy obsłużyć słowa spoza słownika zamieniająć na token `UNK` słowa występujące jednokrotnie w zbiorze danych. Dodatkowo słownik powinien uwzględniać token specjalny `<PAD>`, najlepiej umieszczony pod indeksem 0, który będzie potrzebny w dalszej części ćwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, seq_texts, seq_tags):\n",
    "        self.vocab_text = build_vocab_from_iterator(seq_texts, specials=[\"<pad>\", \"<unk>\"], min_freq=2)\n",
    "        self.vocab_text.set_default_index(1)\n",
    "\n",
    "        self.vocab_tag = build_vocab_from_iterator(seq_texts, specials=[\"<pad>\", \"<unk>\"], min_freq=2)\n",
    "        self.vocab_tag.set_default_index(1)\n",
    "\n",
    "        self.texts = [self.vocab_text(txt) for txt in seq_texts]\n",
    "        self.labels = [self.vocab_tag(tag) for tag in seq_tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text': self.texts[item], 'label': self.labels[item]}\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab_text)\n",
    "\n",
    "    def get_tagset_size(self):\n",
    "        return len(self.vocab_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = CoNLLDataset(seq_texts, seq_tags)\n",
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mając zaimplementowany zbiór danych jako obiekt typu `Dataset`, podzielenie go na paczki instancji nie jest trudne. Wystarczy wykorzystać obiekt `DataLoader` i wyspecyfikować w nim rozmiar paczki danych `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "print(\"Ile utworzono paczek?\", len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Niemniej jednak praca z tekstami nie jest niestety taka prosta spróbuj przeiterować pod kolejnych paczkach twojego zbioru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i in dataloader: TODO: UNCOMMENT\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Obiekt `DataLoader` próbuje automatycznie podzielić nasze dane na paczki, jednak nie jest to takie proste. Nasze dane składają się z sekwencji o różnych długościach, ciężko jest więc utworzyć z nich eleganckie tensory które mają stałe wymiarowości. Naturalnym rozwiązaniem problemu jest wyznaczenie długości najdłuższej sekwencji i uzupełnienie wszystkich pozostałych sekwencji do tej długości specjlanymi tokenami `<PAD>`. Taka operacja może niestety znacznie zwiększyć wymagania pamięciowe potrzebne do reprezentacji zbioru. Pojawienie się jednej bardzo długiej sekwencji w zbiorze znacznie zwiększa czas przetwarzania dla wszystkich jego elementów. \n",
    "\n",
    "Zauważ, że wymaganiem technicznym nie jest posiadanie całego zbioru danych w postaci jednego dużego tensora, ale stworzenie takiego tensora dla wszystkich sekwnencji w ramach jednej paczki danych. Zwykle długość najdłużej sekwencji w paczce jest dużo mniejsza niż długość najdłuszej sekwencji w całym zbiorze, co pozwoliłoby nam na nie tylko znaczne lepsze wykorzystanie pamięci operacyjnej, ale także na ograniczenie czasu przetwarzania. Z tego powodu w tym zadaniu będziemy dynamicznie tworzyć reprezentację paczki danych -- każda paczka będzie zawierała tyle samo sekwencji, jednak będą one uzupełniane do różnych długości, pod tym względem paczki danych *nie* będą równe.  \n",
    "\n",
    "Do uzupełnienia sekwencji do równych długości przydatna będzie funkcja `pad_sequence`, która również posiada parametr `batch_first` przygotowując dane o odpowiednich wymiarach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "data = [torch.tensor([1, 2, 3]), torch.tensor([1, 2])]\n",
    "pad_data = pad_sequence(data, padding_value=0)\n",
    "print(pad_data)\n",
    "\n",
    "pad_data = pad_sequence(data, padding_value=0, batch_first=True)\n",
    "print(pad_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Warto zwrócić uwagę, że zrównoleglanie przetwarzania sieci rekurencyjnej nie jest proste, gdyż wyniki przetwarzania zależą od poprzednich wyników. Nie można więc, tak jak w przypadku sieci splotowej, równocześnie obliczyć wyników filtra na wszystkich słowach w tekście, znakomicie zrównoleglając przetwarzanie. Należy najpierw zastosować \"filtr\" na pierwszym słowie, poczekać na wynik, potem na kolejnym itd. W związku z tym najlepszą możliwością zrównoleglenia przetwarzania sieci rekurencyjnych jest obsługa wielu przykładów uczących na raz. Sieć rekurencyjna, inicjalizowana potencjalnie różnymi `h0` dla różnych przykładów uczących, na raz przetwarza pierwsze elementy każdej sekwencji. Następnie przetwarza wszystkie drugie elementy, wszystkie trzecie elementy itd. Przetwarzanie odbywać się będzie po kolei w ramach kolejnych wierszy domyślnej repezentacji `batch_first=False` tj. równocześnie przetwarzane będą wszystkie sekwencje w batchu.\n",
    "\n",
    "Implementacja dynamicznego tworzenia paczek danych (wraz z uzupełnianiem `<PAD>`) jest możliwa dzięki przekazaniu do konstruktora `DataLoader` funkcji `collate_fn`. Funkcja ta na wejście otrzymuje kolekcję danych (w takiej postaci w jakiej zostały one zwrócone z `DataSet` a na wyjście powinna zwrócić tensory reprezentujące paczkę danych. Nasza funkcja `collate_fn` powinna zwrócić 3 tensory. Pierwszy tensor powinien zawierać przykłady uczące (o równej długości), drugi odpowiednio uzupełnione sekwencje tagów oraz trzeci tensor jednowymiarowy (wektor) zawierający długości kolejnych sekwencji w batchu.\n",
    "\n",
    "*UWAGA*: Konwencją umożliwiającą potem efektywniejsze przetwarzanie jest to, że sekwencje w batchu są posortowane ich długościami. Pierwsza sekwencja powinna być najdłuższa, a ostatnia najkrótsza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x['text']), reverse=True)\n",
    "\n",
    "    txt_tensor = []\n",
    "    lbl_tensor = []\n",
    "    info_tensor = []\n",
    "\n",
    "    for b in batch:\n",
    "        txt_tensor.append(torch.tensor(b['text']))\n",
    "        lbl_tensor.append(torch.tensor(b['label']))\n",
    "        info_tensor.append(len(b['text']))\n",
    "\n",
    "    # txt_tensor = [torch.tensor(x['text']) for x in batch]\n",
    "    # lbl_tensor = [torch.tensor(x['label']) for x in batch]\n",
    "    # info_tensor = [len(x) for x in txt_tensor]\n",
    "\n",
    "    txt_tensor = pad_sequence(txt_tensor, padding_value=0, batch_first=True)\n",
    "    lbl_tensor = pad_sequence(lbl_tensor, padding_value=0, batch_first=True)\n",
    "\n",
    "    return txt_tensor, lbl_tensor, info_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
    "for i in dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Uzupełnienie sekwencji tokenami `<PAD>` do pełnej długości wydaje sie być dobrym pomysłem i pozwala na operowanie na tensorach które łatwo przesłać na kartę graficzną czy wykonać równoległe obliczenia na wszystkich jego elementach. Niemniej jednak fakt, że nasze sekwencje nie są równiej długości jest nadal niezwykle istotny przy wielu różnych obliczeniach. W szczególności jest on istotny przy przetwarzaniu sekwencji przez sieć rekurencyjną. W przypadku sieci rekurencyjnej przetwarzającej sekwencję uzupełnioną do końca zerami, sieć nie zwróci reprezentacji dla każdego słowa ale dla wszystkich elementów sekwencji czyli także dla końcowych elementów-zer. Ewidentnie sieć wykonała wiele niepotrzebnych operacji obliczeniowych tracąc czas (a przecież sieci rekurencyjne do najszybszych nie należą) jednakże prawidłowy wynik nadal jest do odzyskania. Możemy przecież post-factum wziąć pod uwagę tylko (długość sekwencji)-pierwszych elementów wyniku, uzyskując taki sam wynik jak przy przetwarzaniu sekwencji bez uzupełnienia.\n",
    "\n",
    "Tak się jednak nie dzieje, gdy rozważamy dwukierunkową sieć rekurencyjną. Sieć iterująca w tył rozpoczęła przetwarzanie od początkowego stanu ukrytego, a następnie akutalizowała go za kolejne wejścia uważając sekwencję tokenów `<PAD>`! Wynik obliczeń tej sieci jest więc różny niż dla sekwencji nieuzupełnionej i co więcej nie jest on do odzyskania. Uzupełniając sekwnecję np. zerami nie tylko wykonujemy nadmiarowe obliczenia, ale także zaburzamy wyniki.\n",
    "\n",
    "Na szczęście sieci rekurencyjne zaimplementowane w PyTorch obsługją także specjalny format danych tzw. `PackedSequence`, który przechowuje informację o długościach sekwencji zapewniając oszczędność obliczeń i takie same wyniki jak dla sekwencji bez uzupełniania. Przed wykonaniem obliczeń siecią rekurencyjną możemy dane zapakować do tego formatu, a następnie przetworzony wynik możemy z powrotem odpakować do postaci uzupełnionego do pełnej długości tensora. Pomocne są w tym dwie funkcje: `pack_padded_sequence` pakująca dane do tego formatu oraz funkcja odpakowująca `pad_packed_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "data = torch.tensor([[1, 2, 3, 4, 5], [-1, -2, -3, 0, 0], [10, 20, 0, 0, 0]])\n",
    "lengths = torch.tensor([5, 3, 2])\n",
    "\n",
    "packed_data = pack_padded_sequence(data, lengths, batch_first=True)\n",
    "print(packed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zapakowaliśmy 3 sekwencje, które uzupełnione były do długości 5. Ponieważ kolejne sekwencje są umieszczone w kolejnych wierszach tensora należy wyspecyfikować argument `batch_first=True`. Format `PackedSequence` przechowuje tensor `data` oraz tensor `batch_sizes`. Tak jak opisywaliśmy, sieć rekurencyjna zrównolegla swoje działania poprzez jednoczene przetwarzanie wszystkich sekwencji na raz, iterując po ich kolejnych elementach. Z tego powodu `batch_sizes` opisują wielkość przetwarzanej paczki danych (tj. liczba przetwarzanych sekwencji) w każdej iteracji. Na początku przetwarzamy 3 sekwencje, potem znowu 3, a następnie tylko dwie, gdyż ostatnia sekwencja miała tylko 2 elementy. Prześledź, że właśnie w taki sposób zostały ułożone dane w `data`.\n",
    "\n",
    "Porównajmy działanie neuronu rekurencyjnego dla danych uzupełnionych tokenami `<PAD>` oraz dla danych spakowanych do `PackedSequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_FEATURES = 1\n",
    "N_LAYERS = 1\n",
    "HIDDEN_SIZE = 1\n",
    "\n",
    "data = torch.FloatTensor([[1, 2, 3, 4, 5], [-1, -2, -3, 0, 0], [10, 20, 0, 0, 0]]).view(3, 5, 1)\n",
    "lengths = torch.tensor([5, 3, 2])\n",
    "\n",
    "rnn = nn.RNN(N_FEATURES, HIDDEN_SIZE, N_LAYERS, batch_first=True)\n",
    "out, _ = rnn(data)\n",
    "print(\"Wyjście sieci dla danych o stałej długości: \", out)\n",
    "\n",
    "packed_data = pack_padded_sequence(data, lengths, batch_first=True)\n",
    "print(data.shape, packed_data[0].shape)\n",
    "out_packed, _ = rnn(packed_data)\n",
    "print(\"Wyjście sieci dla danych spakowanych: \", out_packed)\n",
    "out, out_len = pad_packed_sequence(out_packed, batch_first=True)\n",
    "print(\"Wyjście sieci po rozpakowaniu: \", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zwróć uwagę jak ułożone są spakowane wyniki sieci.\n",
    "\n",
    "Stworzyliśmy zbiór danych, mamy także zaimplementowany mechanizm dynamicznych paczek danych, a także wiemy jak efektywnie wykorzystywać je do obliczeń siecami rekurencyjnymi. Połóżmy więc wisienkę na torcie i stwórzmy model do tagowania tych danych. Model powinien składać się z warstwy zanurzeń słów, która jest wejściem do jednowarstwowego, dwukierunkowego LSTM. Ostatecznie predykcja jest wykonywana przez warstwę liniową (softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "WORD_EMBEDDING = 20\n",
    "\n",
    "\n",
    "class TaggerNet(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_tags):\n",
    "        \"\"\" Argumentami konstruktora jest rozmiar słownika, \n",
    "            liczba neuronów w warstwie rekurencyjnej \n",
    "            oraz liczba klas/tagów\n",
    "        \"\"\"\n",
    "        super(TaggerNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "\n",
    "        self.lstm = nn.LSTM(WORD_EMBEDDING, hidden_size, n_tags, batch_first=True, bidirectional=True)\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "    def forward(self, sentence, seq_lengths):\n",
    "        \"\"\" Wejściem jest paczka zdań (słowa reprezentowane przez indeksy)\n",
    "            oraz tensor ich długości \n",
    "        \"\"\"\n",
    "        emb = self.embedding(sentence)\n",
    "        print(\"EMB\")\n",
    "\n",
    "        packed_data = pack_padded_sequence(emb, seq_lengths, batch_first=True)\n",
    "        out_packed, h = self.lstm(packed_data)\n",
    "        print(\"LSTM\")\n",
    "        out, out_len = pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "        out = self.soft(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = TaggerNet(dataset.get_vocab_size(), 10, dataset.get_tagset_size())\n",
    "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss_sum = torch.tensor(0.)\n",
    "    for sentences, tags, lengths in dataloader:\n",
    "        pred = model(sentences, lengths)\n",
    "        # print(pred.shape, tags.shape)\n",
    "        # pred = pred.mean(dim=0)\n",
    "        # pred = torch.reshape(pred, (pred.shape[0], pred.shape[1]))\n",
    "        pred = pred.swapaxes(1, 2)\n",
    "        # print(pred.shape, tags.shape)\n",
    "\n",
    "        loss = loss_function(pred, tags)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.5)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += loss\n",
    "    print(loss_sum / len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zaimplemetuj finalną pętlę uczącą model. \n",
    "- Powinieneś wykorzystać błąd entropii krzyżowej jako funkcję straty policzoną dla każdego przewidzianego taga. \n",
    "- Zwróć uwagę, że wyniki sieci są w postaci rozpakowanej tj. uzupełnionej zerami do pełnej długości. Błąd nie powinien być liczony dla tych predykcji! Możesz to osiągnąć np. sprytnie wykorzystując parametr `ignore_index` klasy `nn.CrossEntropyLoss`. \n",
    "- Jako algorytm optymalizacyjny wykorzystaj `torch.optim.Adam`.\n",
    "- Jedną z technik stabilizujących trening sieci jest przycinanie gradientu. W PyTorch możesz to uzyskać wywołując pomiędzy obliczeniem gradientu a wywołaniem kroku optymalizatora funkcji `nn.utils.clip_grad_norm_(model.parameters(), TRESHOLD)`. Zwróć uwagę, że nazwa funkcji zakończona jest `_` to znaczy, że operacja jest wykonywana in-place i wyniku funkcji nie trzeba podstawiać do żadnej zmiennej.\n",
    "- Celem ćwiczenia nie jest wybór hiperparametrów, ani uzyskiwanie wysokich trafności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "#\n",
    "# model = TaggerNet(dataset.get_vocab_size(), 10, dataset.get_tagset_size())\n",
    "# dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn)\n",
    "#\n",
    "# loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "# optimizer = Adam(model.parameters())\n",
    "#\n",
    "# for epoch in range(5):\n",
    "#     loss_sum = torch.tensor(0.)\n",
    "#     for sentences, tags, lengths in dataloader:\n",
    "#         pred = model(sentences, lengths)\n",
    "#\n",
    "#         loss = loss_function(pred, tags)\n",
    "#         loss.backward()\n",
    "#\n",
    "#         # nn.utils.clip_grad_norm_(model.parameters())\n",
    "#\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#         loss_sum += loss\n",
    "#     print(loss_sum / len(dataloader))\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Ćwiczenia**\n",
    "- W tym zadaniu zaimplementowaliśmy dynamiczne tworzenie paczek danych, w ramach których uzupełnialiśmy sekwencje do takiej samej długości. Czasami stosowaną techniką jest posortowanie zbioru danych, tak aby sekwencje były ułożone od najdłuższej do najkrótszej, a następnie dynamicznie tworzy się kolejne paczeki danych iterując po tak ułożonym zbiorze. Jakie są wady i zalety takiego rozwiązania?\n",
    "- Student podczas treningu sieci neuronowej zauważył, że wyniki funkcji celu w czasie optymalizacji modelu nie są stabilne. Spodziewa się, że zwiększenie rozmiaru paczki danych przyczyni się do bardziej stabilnego treningu sieci i poprawi wyniki. Jednakże, dalsze zwiększenie paczki danych nie jest możliwe gdyż nie mieści się ona w pamięci na karcie GPU. Sama implementacja sieci i reprezentacji zbioru danych jest wg. studenta optymalna, inna karta GPU o większej pamięci nie jest dostępna. Jak rozwiązac ten problem? Podaj zarys implementacji. \n",
    "\n",
    "\n",
    "Odpowiedź na ostatnią kropkę umieść poniżej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TWOJA ODWPOWIEDŹ TUTAJ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}